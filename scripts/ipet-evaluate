#!/usr/bin/env python
'''
The MIT License (MIT)

Copyright (c) 2016 Zuse Institute Berlin, www.zib.de

Permissions are granted as stated in the license file you have obtained
with this software. If you find the library useful for your purpose,
please refer to README.md for how to cite IPET.

@author: Gregor Hendel
'''

from ipet import Experiment
from ipet import Key
import argparse
import sys
from ipet.evaluation import IPETEvaluation

import re
import textwrap
import os

import logging

description = \
'''
    produces a table evaluation of test runs according to an evaluation XML-file

    An evaluation file is an xml file that specifies a number of columns of the original
    log file data and a number of interesting groups of instances for which aggregated
    results of the data should be produced.

    The script produces two tables: the first, instancewise table has all specified columns
    for every passed log file and one row per instance. 
    The second, aggregated table shows aggregated statistics for all specified filter 
    groups for this evaluation.
'''

epilog = \
    '''
    =================
    Examples of Usage
    =================

    The simplest way to invoke the script is to specify the name of a parsed log file and
    the name of a valid evaluation file, e.g.,

       ipet-evaluate -t sometestrun.trn -e evaluation.xml

    A sample evaluation script to start with is '[IPET-ROOT]/scripts/evaluation.xml' which uses only readily 
    available data like the number of solving nodes and the solving time in seconds.


    Key Search
    ----------


    

    '''

validation_doc = """
    Validation
    ----------
    
    IPET can use external problem information to validate collected solver output. External problem information
    should be contained in a solu-file and can be passed by the -v, --validate command line option.
    
    Example
    
    
        >>> ipet-evaluate -t mytestrun.trn -e myevaluation.xml -v subdir/solutions.solu
    
    specifies the solution information 'subdir/solutions.solu' as solution information.
    It is possible to store this information as attribute for 'myevaluation.xml'.
    
    If the working directory contains solu file reference information , and
    no validation is explicitly specified, ipet-evaluate recognizes and uses
    the solu file from the working directory.
"""

output_doc = """

    Output control
    --------------
    
    By default, ipet-evaluate displays aggregated results for every filter group specified
    in an evaluation file. The following command line options can be used
    to control the verbosity of the output:
    
    - -l, --long : Show the 'long' table, with one row for every index element.
    --displaygroup : Specify the name of the filter group (as declared in the evaluation)
        for which the long table
        should be printed. Only useful in combination with '--long'.
    - -q, --quiet : Suppress all output
    - -D, --debug : enable debugging output
"""

# possible arguments in the form name,default,short,description #
clarguments = [('--experimentfile', None, '-x', "An experiment file name (must have .ipx file extension) in ipx-format to read"),
               ('--evalfile', None, '-e', "An evaluation file name (must have .xml file extension) in xml-format to read"),
               ('--externaldata', None, '-E', "Should external data such as additional instance information be used?"),
               ('--defaultgroup', None, '-d', "overwrites the default group specified in the evaluation"),
               ('--compformatstring', None, '-C', "a format string like %%.5f for compare columns (those ending with ...'Q')"),
               # ('--groupkey', None,'-g', "overwrites the group key as, e.g., 'Settings' specified in the evaluation by something else"),
               ('--prefix', None, '-p', "a prefix string for every file written, only useful combined with --filextension"),
               ('--keysearch', None, '-k', "a string containing a regular expression to search all columns that match this expression")]

argparser = argparse.ArgumentParser(prog = "IPET command line evaluation", \
                                 description = description,
                                 epilog = textwrap.dedent(epilog
                                                          + validation_doc
                                                          + output_doc),
                                 formatter_class = argparse.RawDescriptionHelpFormatter)
for name, default, short, description in clarguments:
    argparser.add_argument(short, name, default = default, help = description)

argparser.add_argument('-t', '--testrunfiles', nargs = '*', default = [], help = "list of .trn files that should used for the evaluation")

argparser.add_argument("-A", "--showapp", action = "store_true", default = False, help = "Display the Evaluation Editor app to modify the evaluation")
argparser.add_argument("-l", "--long", action = "store_true", default = False, help = "use for long output (instancewise and aggregated results)")
argparser.add_argument("-D", "--debug", action = "store_true", default = False, help = "Enable debug output to console during parsing")
argparser.add_argument('-s', '--saveexperiment', action = "store_true", default = False, help = "Should the experiment data be overwritten? Makes only sense if combined with '--recollect True'")
argparser.add_argument('-r', '--recollect', action = "store_true", default = False, help = "Should the loaded experiment recollect data before proceeding?")
argparser.add_argument('-i', '--index', action = "append", default = None, help = "specification of (multilevel) index seperated by whitespaces")
argparser.add_argument('--indexsplit', default = None, help = "position to split index into row and column levels, negative to count from the end.")
argparser.add_argument('--quiet', action = "store_true", default = False, help = "Supress all output (may be useful for profiling)")
argparser.add_argument('--displaygroup', default = None, help = "Name of the group for which the long display should be printed. Only available for long output mode")
argparser.add_argument('-v', '--validate', default = None, help = "Name of a solu file for validation information")
argparser.add_argument('-f', '--fileextensions', default = None, action = "append", help = "file extensions for writing evaluated data, e.g., csv, tex, stdout, txt")
argparser.add_argument('--tolerance', default = None, help = "relative objective tolerance for validation")

if __name__ == '__main__':
    try:
        arguments = argparser.parse_args()
    except:
        if not re.search(" -+h", ' '.join(sys.argv)) :
            print("Wrong Usage, use --help for more information.")
        exit()

    # if globals().get("help") is not None:
    logger = logging.getLogger()
    formatter = logging.Formatter('%(asctime)s - %(levelname)7s - %(name)s - %(message)s')
    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(formatter)
    logger.addHandler(ch)
    if arguments.quiet:
        logger.setLevel(logging.ERROR)
    elif arguments.debug:
        logger.setLevel(logging.DEBUG)
    else:
        logger.setLevel(logging.INFO)
    # initialize an experiment
    experiment = None
    if arguments.evalfile is None:
        evalfile = None
        # search for files in the current directory that might contain an evaluation, stop with the first evaluation
        for _file in os.listdir("./"):
            if _file.endswith(".xml") or _file.endswith(".ipe"):
                try:
                    _ = IPETEvaluation.fromXMLFile(_file)
                    evalfile = _file
                    logger.info("No eval-file specified, using evaluation %s from current directory" % evalfile)
                    break
                except Exception as e:
                    print(e)
                    continue
        # take the standard evaluation, if no evaluation could be found in current directory
        if evalfile is None:
            evalfile = os.path.join(os.path.dirname(__file__), 'evaluation.xml')
            try:
                _ = IPETEvaluation.fromXMLFile(evalfile)
            except Exception as e:
                print(e)
                logger.info("No eval-file specified, and standard evaluation %s could not be loaded -- Stopping" % evalfile)
                sys.exit(0)
            logger.info("No eval-file specified, using standard evaluation %s" % evalfile)
    else:
        evalfile = arguments.evalfile




    if arguments.experimentfile is None and arguments.testrunfiles == []:
        print("Please provide either an experimentfile or (multiple, if needed) .trn testrun files")
        sys.exit(0)
    theeval = IPETEvaluation.fromXMLFile(evalfile)

    # override unspecified validation
    if not arguments.validate and not theeval.getValidate():
        for _file in os.listdir("./"):
            if _file.endswith(".solu"):
                validate = _file
                theeval.set_validate(validate)
                logger.info("No validation information specified, using '{}' from working directory".format(validate))
                break
    elif arguments.validate:
        theeval.set_validate(arguments.validate)

    if arguments.tolerance:
        theeval.set_gaptol(float(arguments.tolerance))

    if arguments.experimentfile is not None:
        experiment = Experiment.loadFromFile(arguments.experimentfile)
    else:
        experiment = Experiment()

    for trfile in arguments.testrunfiles:
        experiment.addOutputFile(trfile)

    if arguments.recollect is not False:
        logger.info("Recollecting data")
        experiment.collectData()
        # TODO What was it about the validation?
        # if arguments.recheckTestrun:
        #    experiment.checkProblemStatus()

    if arguments.saveexperiment is not False:
        experiment.saveToFile(arguments.experimentfile)

    if arguments.indexsplit is not None:
        theeval.set_indexsplit(arguments.indexsplit)

    if arguments.index is not None:
        theeval.set_index(arguments.index)

    if arguments.defaultgroup is not None:
        # theeval.setDefaultGroup(arguments.defaultgroup)
        #
        # this will only work from our test directory using the following command
        #
        # ipet-evaluate -t results/check.MMMc.scip-lns.linux.x86_64.gnu.opt.cpx.none.M610.gamma_*_beta_0*.trn -e eval-test.xml -l -n -d 0.01 -D
#        theeval.setDefaultGroup((0.10, 0.05))
        theeval.set_defaultgroup(arguments.defaultgroup)

#    if arguments.groupkey is not None:
#        theeval.setGroupKey(arguments.groupkey)

    if arguments.externaldata is not None:
        experiment.addExternalDataFile(arguments.externaldata)
        logger.info("Added external data file %s" % arguments.externaldata)
    else:
        logger.info("No external data file")
        experiment.externaldata = None

    if arguments.compformatstring is not None:
        theeval.setCompareColFormat(arguments.compformatstring)

    if arguments.keysearch is not None:
        logger.info("Starting key enumeration")
        for key in experiment.getDatakeys():
            if re.search(arguments.keysearch, key):
                logger.info("    " + key)
        logger.info("End key enumeration")
        exit(0)

    if arguments.showapp:
        from ipetgui import IpetEvaluationEditorApp, ExperimentManagement
        from PyQt4.Qt import QApplication

        application = QApplication(sys.argv)
        application.setApplicationName("Evaluation editor")
        mainwindow = IpetEvaluationEditorApp()
        try:
            mainwindow.setEvaluation(theeval)
        except:
            pass
        ExperimentManagement.setExperiment(experiment)
        mainwindow.setExperiment(ExperimentManagement.getExperiment())

        theeval.evaluate(ExperimentManagement.getExperiment())
        mainwindow.show()

        sys.exit(application.exec_())

    # returntable and returnaggregation
    rettab, retagg = theeval.evaluate(experiment)

    if not arguments.quiet:
        if arguments.long:
            if arguments.displaygroup is not None:
                for fg in theeval.getActiveFilterGroups():
                    if fg.getName() == arguments.displaygroup:
                        theeval.streamDataFrame(theeval.getInstanceGroupData(fg), "Instancewise Results", "stdout")
            else:
                theeval.streamDataFrame(rettab, "Instancewise Results", "stdout")

        theeval.streamDataFrame(retagg, "Aggregated Results", "stdout")

    # for tr in comp.testrunmanager.getManageables():
        # for col in tr.data.columns:
            # print col

    if arguments.fileextensions is not None:

        path = "."
        prefixstr = arguments.prefix if arguments.prefix else ""


        for extension in arguments.fileextensions:
            for fg in theeval.getActiveFilterGroups():
                instancewisename = "%s/%s%s" % (path, prefixstr, fg.name)

                theeval.streamDataFrame(theeval.getInstanceGroupData(fg), instancewisename, extension)
                logger.info("Instance-wise data written to %s.%s" % (instancewisename, extension))
                aggname = instancewisename + "_agg"
                theeval.streamDataFrame(theeval.getAggregatedGroupData(fg), aggname, extension)
                logger.info("aggregated data written to %s.%s" % (aggname, extension))

            filename = "%s/%s" % (path, "_".join((prefixstr, "inst_combined")) if prefixstr != "" else "inst_combined")
            theeval.streamDataFrame(rettab, filename, extension)
            logger.info("combined instance data written to %s.%s" % (filename, extension))
            # print the combined aggregated data if there are multiple filter groups present
            filename = "%s/%s" % (path, "_".join((prefixstr, "agg_combined")) if prefixstr != "" else "agg_combined")
            theeval.streamDataFrame(retagg, filename, extension)
            logger.info("combined aggregated data written to %s.%s" % (filename, extension))

    # print pd.concat([rettab, theeval.levelonedf], axis=1)

