#!/usr/bin/env python
'''
The MIT License (MIT)

Copyright (c) 2016 Zuse Institute Berlin, www.zib.de

Permissions are granted as stated in the license file you have obtained
with this software. If you find the library useful for your purpose,
please refer to README.md for how to cite IPET.

@author: Gregor Hendel
'''

from ipet import Experiment
from ipet.parsing import ReaderManager
from ipet import TestRun
from ipet.misc import loader
import argparse
import sys
import os
import io
import re
import logging
#from IPython.utils.text import dedent
import textwrap
import pypandoc

DEFAULT_FORMATSTR = "{idx} {d}"

formatstrExamples = """
Format String Examples
----------------------

With the -f option, a format string can be specified to control the parsing output. Normal Python format syntax
with curly braces '{}' is used for this. The default value for the format string is '{idx} {d}', which will
output the index of every instance together with its complete data series. 'd' stands for 'data'. By accessing the elements of
'd' with the format string, only relevant information is displayed on the console.

    >>> cat test/data/scip-*.out | ipet-parse -f "{idx} {d.ProblemName} {d.SolvingTime}"

should yield

    0 enlight16 0.02
    1 janos-us-DDM 6807.85
    2 misc03 0.79
    3 aflow40b 600.0

To make the information more readible, the syntax has further options for formatting:

    >>> cat test/data/scip-*.out | ipet-parse -f "{idx:3} {d.ProblemName:40} {d.SolvingTime:12}"

    0 enlight16                                        0.02
    1 janos-us-DDM                                  6807.85
    2 misc03                                           0.79
    3 aflow40b                                        600.0
"""
explainAutoloading = """
Loading additional files
------------------------

Ipet-parse can load additional .solu-files, .meta-files, Readers specified via xml and Solvers specified as python classes.

    - Solvers are automatically loaded from the ~/.ipet/solvers directory.
        They have to be derived from the 'Solver' class and implement some basic methods.
    - Readers are automatically loaded from the current and the ~/.ipet/readers directories.
        A (list of) readers can also be passed via the commandline -r parameter.
        They have to be specified via xml, i.e. via the graphical user interface.
    - .solu-files are loaded from the current directory if none are specified via the commandline -s parameter.
        If no solu-file is found in the current directory, IPET loads them from ~/.ipet/solufiles.
    - For .meta-files IPET simply substitutes for every logfile the extension .out with .meta and tries to load it from the same location.
"""

# possible arguments in the form name,default,short,description #
# clarguments = []

argparser = argparse.ArgumentParser(prog = "Ipet Parsing script", \
                                 description = "parses test run log files and saves parsed data",
                                 epilog = textwrap.dedent(formatstrExamples + explainAutoloading),
                                 formatter_class = argparse.RawDescriptionHelpFormatter)
# for name, default, short, description in clarguments:
#     argparser.add_argument(short, name, default = default, help = description)

argparser.add_argument('-l', '--logfiles', nargs = '*', help = "list of outfiles that should be parsed", default=sys.stdin)
argparser.add_argument('-r', '--readers', nargs = '*', default = [], help = "list of additional readers in xml format that should be used for parsing")
argparser.add_argument('-s', '--solufiles', nargs = '*', default = [], help = "list of solu files that should be taken into account")
argparser.add_argument("-D", "--debug", action = "store_true", default = False, help = "Enable debug output to console during parsing")
argparser.add_argument("-a", "--all_rows", action = "store_true", default = False, help = "In piped moded: display all rows.")
argparser.add_argument("-f", "--formatstr", default = DEFAULT_FORMATSTR,
                       help = """In piped mode: format string for displaying output per instance to console. The internal formatter uses 'idx'
                                   to reference the index and 'd' for the collected data for this instance. default : "%s" """ % DEFAULT_FORMATSTR
                                   )
argparser.add_argument("-v", "--validatedual", action = "store_true", default = Experiment.DEFAULT_VALIDATEDUAL, help = "Enable dual validation, relative to 'gaptol' parameter")
argparser.add_argument("-g", "--gaptol", type = float, default = Experiment.DEFAULT_GAPTOL, help = "relative tolerance for primal and dual objective validation")
argparser.add_argument("--docmode", action = "store_true", default = False, help = "print this help as restructured text")
argparser.add_argument("--csv", action = "store_true", default = False, help = "print the data as csv")
argparser.add_argument("-j", "--jobs", default = -1, type=int, help = "number of threads to use (-1 for all available threads)")

def setup_experiment(arguments, verbose=False):
    experiment = Experiment(validatedual = arguments.validatedual, gaptol = arguments.gaptol)
    for reader in loader.loadAdditionalReaders(arguments.readers):

        experiment.readermanager.registerReader(reader)
        if verbose:
            logger.info("Registered reader with name %s" % reader.getName())

    for solver in loader.loadAdditionalSolvers():
        experiment.readermanager.addSolver(solver)
        if verbose:
            logger.info("Registered solver with name %s" % solver.getName())

    for solufile in loader.loadAdditionalSolufiles(arguments.solufiles):
        experiment.addSoluFile(solufile)
        if verbose:
            logger.info("Imported solufile with name %s" % solufile)

    return experiment

def process_one_outfiles_group(task : tuple):
    # initialize an experiment
    outfiles, arguments = task
    experiment = setup_experiment(arguments)

    logger.info("Start parsing process of outfile(s) {}".format(", ".join(outfiles)))

    for o in outfiles:
        experiment.addOutputFile(o)

    experiment.collectData()

    # Write output
    for tr in experiment.getTestRuns():
        try:
            filename = tr.filenames[0]
            newfilename = "%s%s" % (os.path.splitext(filename)[0], TestRun.FILE_EXTENSION)
            tr.saveToFile(newfilename)
            logger.info("converted %s --> %s" % (filename, newfilename))
        except:
            logger.info("skipped testrun %s" % tr.getIdentification())

        if arguments.csv:
            try:
                filename = tr.filenames[0]
                newfilename = "%s%s" % (os.path.splitext(filename)[0], ".ipetdata.csv")
                tr.saveToCSV(newfilename)
                logger.info("saved data for %s --> %s" % (filename, newfilename))
            except:
                logger.info("couldn't save data for testrun %s" % tr.getIdentification())

    return outfiles

if __name__ == '__main__':

    try:
        arguments = argparser.parse_args()
    except:
        if not re.search(" -+h", ' '.join(sys.argv)) :
            print("Wrong Usage, use --help for more information.")
        exit()
    # if globals().get("help") is not None:
    logger = logging.getLogger()
    formatter = logging.Formatter('%(asctime)s - %(levelname)7s - %(name)s - %(message)s')
    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(formatter)
    logger.addHandler(ch)

    if arguments.docmode:
        print(pypandoc.convert_text(formatstrExamples, "rst", "md"))
        exit()

    if arguments.debug:
        logger.setLevel(logging.DEBUG)
    else:
        logger.setLevel(logging.INFO)
    # disable logging to console, in this case we just want the printstatement at the end
    if type(arguments.logfiles) == io.TextIOWrapper and not arguments.debug:
        logger.setLevel(logging.ERROR)


    # initialize an experiment
    experiment = setup_experiment(arguments, verbose=True)


    if type(arguments.logfiles) != io.TextIOWrapper:
        import multiprocessing as mp
        import tqdm

        nthreads = mp.cpu_count() if arguments.jobs == -1 else arguments.jobs
        pool = mp.Pool(nthreads)
        logger.info("Start parsing process using {} threads".format(nthreads))

        # group the input files by their base names, to process related out and err files together
        # META files are automatically picked up internally upon creation of a TestRun object
        logfiles2basename = {}
        for f in arguments.logfiles:
            logfiles2basename.setdefault(os.path.splitext(f)[0], []).append(f)


        tasks = [(outfiles, arguments) for outfiles in logfiles2basename.values()]
        for _ in tqdm.tqdm(pool.imap_unordered(process_one_outfiles_group, tasks), total=len(tasks)):
            pass

        pool.close()
        pool.join()

    else:
        experiment.addStdinput()
        experiment.collectData()
        experiment.printToConsole(arguments.formatstr, arguments.all_rows)
